=================================================
Introduction
=================================================

This is the documentation of the Mini Places Challenge development kit.

Table of contents:
  1. Overview of challenge dataset
  2. Challenge details
    2.1 Images and annotations
    2.2 Submission format
    2.3 Evaluation routines

Prepared by: Aditya Khosla

Please contact the course staff for any questions, comments or bug reports.

=================================================
1. Overview of challenge dataset
=================================================

There are three types of image data for this competition, all coming
from the larger Places2 dataset: training data (TRAINING), validation 
data (VALIDATION), and test (TEST). There is no overlap in the three 
sources of data: TRAINING, VALIDATION, and TEST.  All three sets of 
data contain images of 100 categories of scenes.

                 Number of images

    Dataset      TRAIN      VALIDATION     TEST
  ------------------------------------------------
   Mini Places  100,000      10,000       10,000

Every image in training, validation and test sets has a single
image-level label specifying the presence of one object category

Challenge database statistics:

  Training:
    
    - 100,000 images, with 1000 images per category

  Validation:

    - 10,000 images, with 100 images per category

  Test:

    - 10,000 images, with 100 images per category

Packaging details:

The link for downloading the data can be obtained via the course website:

    http://6.869.csail.mit.edu/fa15/project.html

The 3 sets of images (training, validation and test) are available as 
a single tar archive. All images are in JPEG format. For the challenge,
images have been resized to 128*128 to make the data manageable for all
students.


=================================================
2. Challenge details
=================================================

The 100 scene categories used in the challenge dataset are part of the 
larger Places2 dataset (http://places2.csail.mit.edu).

All the class names and ids are available in:
    data/categories.txt,

where each line contains the scene category name followed by its id
(an integer between 0 and 99).

---------------------------
2.1.1 Training data
---------------------------

Each image is considered as belonging to a particular scene category. 

After untarring the above file, the directory structure should
look similar to the following:
     train/a/abbey/00000000.jpg
     train/a/abbey/00000001.jpg
           ...
     train/y/yard/00000999.jpg      
     train/y/yard/00001000.jpg

In general, each leaf folder contains one scene category. The complete list 
of training images and their mapping to scene category ids is available in:
     data/train.txt
     
All images are in JPEG format.


-----------------------------
2.1.2 Validation data
-----------------------------

There are a total of 10,000 validation images. They are named as

      val/00000001.jpg
      val/00000002.jpg
      ...
      val/00009999.jpg
      val/00010000.jpg

There are 100 validation images for each scene category.

The classification ground truth of the validation images is in 
    data/val.txt,
where each line contains one image filename and its corresponding scene
category label (from 0 to 99).

-----------------------
2.1.3 Test data
-----------------------

There are a total of 10,000 test images. The test files are named as

      test/00000001.jpg
      test/00000002.jpg
      ...
      test/00009999.jpg
      test/00010000.jpg

There are 100 test images for each scene category. The ground truth 
annotations will not be released.

---------------------------
2.1.4 Object annotations
---------------------------

For a subset of the images (3502 train images, and 371 validation images), 
we provide annotations of the common objects contained within that image.
You may use these annotations in any way you see fit to enhance the performance
of your algorithm on the scene recognition task. You are not required to use
them -- the goal here is to provide additional avenues for exploration. You
will not be judged on the quality of your object recognition.

The specific object annotations available are the bounding boxes and polygons
for 175 different object categories in the 3502 train and 371 validation images.
The list of object categories is provided in the development kit:

      data/object_categories.txt

The annotations are provided in the 'objects' folder in the compressed file
containing the image data. The images that have these annotations will have
corresponding xml files in the objects folder. For example, the image
images/train/a/abbey/00000001.jpg will have the object annotations located
at objects/train/a/abbey/00000001.xml. As mentioned above, only a subset of
the images are annotated with objects, so all image files will not have object
annotations.

To facilitate the reading of the object annotations, we provide the following
MATLAB function in the development kit:

     util/VOCreadxml.m

The above function will read the XML file and convert it to a MATLAB structure
containing the following fields:

 - filename: name of the file
 - folder: scene class name
 - class: scene category id
 - objects: a structure containing the bounding box information and polygon
   points delineating the objects. Either of these annotations may be helpful
   depending on how you intend to use them


*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
2.2 Submission format
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

The submission of results on test data will consist of a text file
with one line per image, in the alphabetical order of the image file
names, i.e. from test/00000001.jpg to test/00010000.jpg. Each line 
contains up to 5 detected scenes, sorted by confidence in descending order. 

The format is as follows:

   <filename> <label(1)> <label(2)> <label(3)> <label(4)> <label(5)>

The predicted labels are the scene categories ( integers between 0 and
99).  The number of labels per line must be exactly equal to 5, or it
would lead to an error. The filename is the same as mentioned above,
e.g., 'test/00000001.jpg' and so on.

Example file on the validation data is 

  evaluation/demo.val.pred.txt    


*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*
2.3 Evaluation routines
*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*-*

The Matlab routine for evaluating the submission is

./evaluation/eval_cls.m  

To see an example of using the routines, start Matlab
in the 'evaluation/' folder and type
       demo_eval_cls;

and you will see something similar to the following output:

MINI PLACES SCENE CLASSIFICATION CHALLENGE
pred_file: demo.val.pred.txt
ground_truth_file: ../data/val.txt
# guesses vs cls error
    1.0000    0.9895
    2.0000    0.9791
    3.0000    0.9696
    4.0000    0.9602
    5.0000    0.9525

In this demo, we take top i ( i=1...5) predictions (and ignore the
rest) from your result file and plot the error as a function of the
number of guesses. 

Only the error with 5 guesses will be used to determine the winner.

(The demo.val.pred.txt used here is a synthetic result.)


====================================================================
References
====================================================================

[1] B. Zhou, A. Khosla, A. Lapedriza, A. Torralba and A. Oliva
Places2: A Large-Scale Database for Scene Understanding
Arxiv, 2015 (PDF coming soon)
http://places2.csail.mit.edu

Please contact Aditya Khosla (khosla@mit.edu) for questions, comments,
or bug reports.
